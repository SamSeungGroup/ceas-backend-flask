{"cells":[{"cell_type":"markdown","metadata":{"id":"wTE1C2HkT_Kh"},"source":["# 텍스트 이진 분류 (긍정 or 부정)"]},{"cell_type":"markdown","metadata":{"id":"BUrDN7ih0IWh"},"source":["## 0. 코랩 연결 끊김 방지"]},{"cell_type":"markdown","metadata":{"id":"lRMv5-rQ1Lu7"},"source":["```\n","function ClickConnect(){\n","  console.log(\"Connnect Clicked - Start\"); \n","  document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click();\n","  console.log(\"Connnect Clicked - End\"); \n","};\n","var clickid = setInterval(ClickConnect, 60*1000)\n","```"]},{"cell_type":"markdown","metadata":{"id":"PdU-y2GL1MOf"},"source":["```\n","clearInterval(clickid);\n","```"]},{"cell_type":"markdown","metadata":{"id":"hf-Gm1MYUGE1"},"source":["## 1. Colab 환경 설정"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IgOjrmhvU0XO"},"outputs":[],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"cell_type":"markdown","metadata":{"id":"WciFP9XbZc3c"},"source":["## 2. 라이브러리 설치"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"maoERJtdjUQN"},"outputs":[],"source":["%cd /content/drive/MyDrive/Colab Notebooks/sentiment-classification"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":116151,"status":"ok","timestamp":1677726067713,"user":{"displayName":"Wooseok Kim (Wooseok)","userId":"10829587564796253131"},"user_tz":-540},"id":"ULpmrNJyZxwv","outputId":"0eaaa5d7-2283-46e8-a7d4-f98836449203"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: mxnet in /usr/local/lib/python3.8/dist-packages (1.7.0.post2)\n","Requirement already satisfied: requests<3,>=2.20.0 in /usr/lib/python3/dist-packages (from mxnet) (2.22.0)\n","Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.8/dist-packages (from mxnet) (1.22.2)\n","Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from mxnet) (0.8.4)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 23.0.1 is available.\n","You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: gluonnlp in /usr/local/lib/python3.8/dist-packages (0.10.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.4.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\n","Requirement already satisfied: cython in /usr/local/lib/python3.8/dist-packages (from gluonnlp) (0.29.33)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from gluonnlp) (21.3)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.8/dist-packages (from gluonnlp) (1.22.2)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2021.3)\n","Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas) (1.14.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->gluonnlp) (3.0.7)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 23.0.1 is available.\n","You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (0.1.96)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 23.0.1 is available.\n","You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n","\u001b[0mCollecting transformers==3\n","  Using cached transformers-3.0.0-py3-none-any.whl (754 kB)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==3) (4.64.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from transformers==3) (1.22.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==3) (3.9.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==3) (2022.10.31)\n","Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers==3) (2.22.0)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (from transformers==3) (0.1.96)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers==3) (21.3)\n","Collecting tokenizers==0.8.0-rc4\n","  Using cached tokenizers-0.8.0rc4-cp38-cp38-manylinux1_x86_64.whl (3.0 MB)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.8/dist-packages (from transformers==3) (0.0.53)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->transformers==3) (3.0.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==3) (1.1.0)\n","Requirement already satisfied: six in /usr/lib/python3/dist-packages (from sacremoses->transformers==3) (1.14.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==3) (8.0.3)\n","Installing collected packages: tokenizers, transformers\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.10.3\n","    Uninstalling tokenizers-0.10.3:\n","      Successfully uninstalled tokenizers-0.10.3\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.8.1\n","    Uninstalling transformers-4.8.1:\n","      Successfully uninstalled transformers-4.8.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","kobert 0.2.3 requires transformers<=4.8.1,>=4.8.1, but you have transformers 3.0.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed tokenizers-0.8.0rc4 transformers-3.0.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 23.0.1 is available.\n","You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.10.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.1.1)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 23.0.1 is available.\n","You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n","\u001b[0mCollecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n","  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-xy_m0lco\n","  Running command git clone --filter=blob:none --quiet 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-xy_m0lco\n","  Resolved https://****@github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: boto3<=1.15.18 in /usr/local/lib/python3.8/dist-packages (from kobert==0.2.3) (1.15.18)\n","Requirement already satisfied: gluonnlp<=0.10.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from kobert==0.2.3) (0.10.0)\n","Requirement already satisfied: mxnet<=1.7.0.post2,>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from kobert==0.2.3) (1.7.0.post2)\n","Requirement already satisfied: onnxruntime<=1.8.0,==1.8.0 in /usr/local/lib/python3.8/dist-packages (from kobert==0.2.3) (1.8.0)\n","Requirement already satisfied: sentencepiece<=0.1.96,>=0.1.6 in /usr/local/lib/python3.8/dist-packages (from kobert==0.2.3) (0.1.96)\n","Requirement already satisfied: torch<=1.10.1,>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from kobert==0.2.3) (1.10.1)\n","Collecting transformers<=4.8.1,>=4.8.1\n","  Using cached transformers-4.8.1-py3-none-any.whl (2.5 MB)\n","Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.8/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (1.22.2)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.8/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (2.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.8/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (3.19.4)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from boto3<=1.15.18->kobert==0.2.3) (0.3.7)\n"]},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: botocore<1.19.0,>=1.18.18 in /usr/local/lib/python3.8/dist-packages (from boto3<=1.15.18->kobert==0.2.3) (1.18.18)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from boto3<=1.15.18->kobert==0.2.3) (0.10.0)\n","Requirement already satisfied: cython in /usr/local/lib/python3.8/dist-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (0.29.33)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (21.3)\n","Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (0.8.4)\n","Requirement already satisfied: requests<3,>=2.20.0 in /usr/lib/python3/dist-packages (from mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.22.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch<=1.10.1,>=1.7.0->kobert==0.2.3) (4.1.1)\n","Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.8/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (0.0.12)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (4.64.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (2022.10.31)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (3.9.0)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Using cached tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (6.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.8/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (0.0.53)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.8/dist-packages (from botocore<1.19.0,>=1.18.18->boto3<=1.15.18->kobert==0.2.3) (2.8.2)\n","Requirement already satisfied: urllib3<1.26,>=1.20 in /usr/lib/python3/dist-packages (from botocore<1.19.0,>=1.18.18->boto3<=1.15.18->kobert==0.2.3) (1.25.8)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (3.0.7)\n","Requirement already satisfied: six in /usr/lib/python3/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (1.14.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (8.0.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (1.1.0)\n","Installing collected packages: tokenizers, transformers\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.8.0rc4\n","    Uninstalling tokenizers-0.8.0rc4:\n","      Successfully uninstalled tokenizers-0.8.0rc4\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 3.0.0\n","    Uninstalling transformers-3.0.0:\n","      Successfully uninstalled transformers-3.0.0\n","Successfully installed tokenizers-0.10.3 transformers-4.8.1\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 23.0.1 is available.\n","You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install mxnet\n","!pip install gluonnlp pandas tqdm\n","!pip install sentencepiece\n","!pip install transformers==3\n","!pip install torch\n","\n","!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"]},{"cell_type":"markdown","metadata":{"id":"LI5j2lzJUtPb"},"source":["## 3. 라이브러리 준비"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q_-Gb8PmZyUz"},"outputs":[],"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import gluonnlp as nlp\n","import numpy as np\n","from tqdm import tqdm, tqdm_notebook\n","\n","from kobert.utils import get_tokenizer\n","from kobert.pytorch_kobert import get_pytorch_kobert_model\n","\n","from transformers import AdamW\n","from transformers.optimization import get_cosine_schedule_with_warmup"]},{"cell_type":"markdown","metadata":{"id":"JcRhktrnJ9Bd"},"source":["## 4. 데이터 준비"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MR3AAmBU1zbO"},"outputs":[],"source":["!git clone https://github.com/e9t/nsmc.git\n","!git clone https://github.com/bab2min/corpus.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pRe-CAglZy1Q"},"outputs":[],"source":["import pandas as pd\n","\n","naver_movie_train_data = pd.read_table(\"nsmc/ratings_train.txt\")\n","naver_movie_test_data = pd.read_table(\"nsmc/ratings_test.txt\")\n","naver_shopping_data = pd.read_table(\"corpus/sentiment/naver_shopping.txt\")\n","steam_data = pd.read_table(\"corpus/sentiment/steam.txt\")"]},{"cell_type":"markdown","metadata":{"id":"sFs0nKnsUynG"},"source":["## 5. 데이터 전처리"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0dvtcV7J2vE9"},"outputs":[],"source":["naver_movie_train_data = naver_movie_train_data.drop(['id'], axis=1)\n","naver_movie_test_data = naver_movie_test_data.drop(['id'], axis=1)\n","\n","naver_shopping_data.columns = ['label', 'document']\n","naver_shopping_data.loc[naver_shopping_data['label'] == 1, 'label'] = 0\n","naver_shopping_data.loc[naver_shopping_data['label'] == 2, 'label'] = 0\n","naver_shopping_data.loc[naver_shopping_data['label'] == 4, 'label'] = 1\n","naver_shopping_data.loc[naver_shopping_data['label'] == 5, 'label'] = 1\n","\n","steam_data.columns = ['label', 'document']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XeeJ3IA12vyb"},"outputs":[],"source":["naver_movie_train_data.drop_duplicates(['document'], inplace=True)\n","naver_movie_test_data.drop_duplicates(['document'], inplace=True)\n","naver_shopping_data.drop_duplicates(['document'], inplace=True)\n","steam_data.drop_duplicates(['document'], inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MrJhO9x-2yTj"},"outputs":[],"source":["naver_movie_train_data = naver_movie_train_data.dropna()\n","naver_movie_test_data = naver_movie_test_data.dropna()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QKiwiUgF20ec"},"outputs":[],"source":["import numpy as np\n","\n","# 한글이 아닌 것 지우기\n","naver_movie_train_data['document'] = naver_movie_train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", regex=True)\n","naver_movie_test_data['document'] = naver_movie_test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", regex=True)\n","naver_shopping_data['document'] = naver_shopping_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", regex=True)\n","steam_data['document'] = steam_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", regex=True)\n","# 공백 모두 지우기\n","naver_movie_train_data['document'] = naver_movie_train_data['document'].str.replace('^ +', \"\", regex=True)\n","naver_movie_test_data['document'] = naver_movie_test_data['document'].str.replace('^ +', \"\", regex=True)\n","naver_shopping_data['document'] = naver_shopping_data['document'].str.replace('^ +', \"\", regex=True)\n","steam_data['document'] = steam_data['document'].str.replace('^ +', \"\", regex=True)\n","# 빈 값 Nan으로 채우기\n","naver_movie_train_data['document'].replace('', np.nan, inplace=True)\n","naver_movie_test_data['document'].replace('', np.nan, inplace=True)\n","naver_shopping_data['document'].replace('', np.nan, inplace=True)\n","steam_data['document'].replace('', np.nan, inplace=True)\n","# Nan값 제거\n","naver_movie_train_data = naver_movie_train_data.dropna()\n","naver_movie_test_data = naver_movie_test_data.dropna()\n","naver_shopping_data = naver_shopping_data.dropna()\n","steam_data = steam_data.dropna()"]},{"cell_type":"markdown","metadata":{"id":"F2LXku8jUy5A"},"source":["## 6. train, test 세트 분리"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v9g99gzdZzsc"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","naver_shopping_train_data, naver_shopping_test_data = train_test_split(naver_shopping_data, test_size=0.25)\n","steam_train_data, steam_test_data = train_test_split(steam_data, test_size=0.25)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gc6MgEWQ3AzB"},"outputs":[],"source":["train_data = pd.concat([naver_movie_train_data, naver_shopping_train_data, steam_train_data])\n","test_data = pd.concat([naver_movie_test_data, naver_shopping_test_data, steam_test_data])\n","\n","train_data = train_data.sample(frac=1).reset_index(drop=True) \n","test_data = test_data.sample(frac=1).reset_index(drop=True) "]},{"cell_type":"markdown","metadata":{"id":"4VQEOjWyUzNj"},"source":["## 7. 데이터 변환"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Tlifvb-3Lb0"},"outputs":[],"source":["##GPU 사용 시\n","device = torch.device(\"cuda:0\")\n","\n","bertmodel, vocab = get_pytorch_kobert_model()\n","\n","tokenizer = get_tokenizer()\n","tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_JG8ULMrZ0L8"},"outputs":[],"source":["class BERTDataset(Dataset):\n","    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n","                 pad, pair):\n","        transform = nlp.data.BERTSentenceTransform(\n","            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair) \n","\n","        self.sentences = [transform([i[sent_idx]]) for i in dataset.itertuples()]\n","        self.labels = [np.int32(i[label_idx]) for i in dataset.itertuples()]\n","\n","    def __getitem__(self, i):\n","        return (self.sentences[i] + (self.labels[i], ))\n","\n","    def __len__(self):\n","        return (len(self.labels))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HHhj-4Np3Trw"},"outputs":[],"source":["# Setting parameters\n","max_len = 64 # 해당 길이를 초과하는 단어에 대해선 bert가 학습하지 않음\n","batch_size = 64\n","warmup_ratio = 0.1\n","num_epochs = 5\n","max_grad_norm = 1\n","log_interval = 200\n","learning_rate = 5e-5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"65OtBPMx3WET"},"outputs":[],"source":["data_train = BERTDataset(train_data, 1, 2, tok, max_len, True, False)\n","data_test = BERTDataset(test_data, 1, 2, tok, max_len, True, False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":295,"status":"ok","timestamp":1677726284715,"user":{"displayName":"Wooseok Kim (Wooseok)","userId":"10829587564796253131"},"user_tz":-540},"id":"TWVz_GnX3VmJ","outputId":"16fef759-fa96-4638-ae80-7e9f9d2345df"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]}],"source":["# pytorch용 DataLoader 사용\n","train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n","test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"]},{"cell_type":"markdown","metadata":{"id":"G3z0Q_zMZuhq"},"source":["## 8. 훈련"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"grkSkdBrZ0kL"},"outputs":[],"source":["class BERTClassifier(nn.Module):\n","    def __init__(self,\n","                 bert,\n","                 hidden_size = 768,\n","                 num_classes = 2, # softmax 사용 <- binary일 경우는 2\n","                 dr_rate=None,\n","                 params=None):\n","        super(BERTClassifier, self).__init__()\n","        self.bert = bert\n","        self.dr_rate = dr_rate\n","                 \n","        self.classifier = nn.Linear(hidden_size , num_classes)\n","        if dr_rate:\n","            self.dropout = nn.Dropout(p=dr_rate)\n","    \n","    def gen_attention_mask(self, token_ids, valid_length):\n","        attention_mask = torch.zeros_like(token_ids)\n","        for i, v in enumerate(valid_length):\n","            attention_mask[i][:v] = 1\n","        return attention_mask.float()\n","\n","    def forward(self, token_ids, valid_length, segment_ids):\n","        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n","        \n","        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n","        if self.dr_rate:\n","            out = self.dropout(pooler)\n","        return self.classifier(out)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tZvSnMXJ3t1B"},"outputs":[],"source":["model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n","\n","# Prepare optimizer and schedule (linear warmup and decay)\n","no_decay = ['bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9gIWt6K73nls"},"outputs":[],"source":["# 옵티마이저 선언\n","optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n","loss_fn = nn.CrossEntropyLoss() # softmax용 Loss Function 정하기 <- binary classification도 해당 loss function 사용 가능"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PeAygM4A3mI7"},"outputs":[],"source":["t_total = len(train_dataloader) * num_epochs\n","warmup_step = int(t_total * warmup_ratio)\n","\n","scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GsBVPjHr3j1j"},"outputs":[],"source":["# 학습 평가 지표인 accuracy 계산 -> 얼마나 타겟값을 많이 맞추었는가\n","def calc_accuracy(X,Y):\n","    max_vals, max_indices = torch.max(X, 1)\n","    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n","    return train_acc"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":244,"referenced_widgets":["4926298a6c934bbd91a442f10bc4f4f9","c0927c341de34c80995d97f641d6edfd","8b09cbbed1b844708dbaf4b61e5a41d9","80e202c50b534af7baac9785d3fb055b","217a062b729844808836afe00a7733ef","74b7d4b4643d43d8893a3b531589e366","7193267578c1462d9092636f34599ad8","b9ce39c5571743149345513ac3200f3c","2ea275c1b82945fc85508f93188506b4","09fe54ac1c744389803369807a8aa7ed"]},"executionInfo":{"elapsed":435,"status":"error","timestamp":1677736021530,"user":{"displayName":"Wooseok Kim (Wooseok)","userId":"10829587564796253131"},"user_tz":-540},"id":"s_6rNwFh3g-E","outputId":"8deaa201-049e-414b-8706-41c4d4ee7b3f","scrolled":false},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_368/1704445530.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4926298a6c934bbd91a442f10bc4f4f9","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5786 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["epoch 1 batch id 1 loss 0.7290350794792175 train acc 0.515625\n","epoch 1 batch id 201 loss 0.6995196342468262 train acc 0.5209888059701493\n","epoch 1 batch id 401 loss 0.5866668820381165 train acc 0.6052447007481296\n","epoch 1 batch id 601 loss 0.32097765803337097 train acc 0.6689631863560732\n","epoch 1 batch id 801 loss 0.41163352131843567 train acc 0.7084698813982522\n","epoch 1 batch id 1001 loss 0.3782798945903778 train acc 0.7335477022977023\n","epoch 1 batch id 1201 loss 0.23876965045928955 train acc 0.7523808284762697\n","epoch 1 batch id 1401 loss 0.2695901393890381 train acc 0.7663610813704497\n","epoch 1 batch id 1601 loss 0.39511433243751526 train acc 0.7769460493441599\n","epoch 1 batch id 1801 loss 0.42404916882514954 train acc 0.7856485980011105\n","epoch 1 batch id 2001 loss 0.42690467834472656 train acc 0.7930019365317341\n","epoch 1 batch id 2201 loss 0.32362300157546997 train acc 0.7995513402998637\n","epoch 1 batch id 2401 loss 0.406591534614563 train acc 0.8052178779675135\n","epoch 1 batch id 2601 loss 0.27320584654808044 train acc 0.8096945886197616\n","epoch 1 batch id 2801 loss 0.3576284348964691 train acc 0.813470635487326\n","epoch 1 batch id 3001 loss 0.37170037627220154 train acc 0.81704535988004\n","epoch 1 batch id 3201 loss 0.27327728271484375 train acc 0.820109926585442\n","epoch 1 batch id 3401 loss 0.3346123993396759 train acc 0.8231126874448692\n","epoch 1 batch id 3601 loss 0.18025809526443481 train acc 0.825716814773674\n","epoch 1 batch id 3801 loss 0.33681249618530273 train acc 0.8284867469087083\n","epoch 1 batch id 4001 loss 0.31696605682373047 train acc 0.8308547863034241\n","epoch 1 batch id 4201 loss 0.3313485383987427 train acc 0.8328671744822661\n","epoch 1 batch id 4401 loss 0.3178550601005554 train acc 0.8346292035900932\n","epoch 1 batch id 4601 loss 0.3828580677509308 train acc 0.8364180341230167\n","epoch 1 batch id 4801 loss 0.31485557556152344 train acc 0.8382758800249948\n","epoch 1 batch id 5001 loss 0.31133896112442017 train acc 0.8398070385922816\n","epoch 1 batch id 5201 loss 0.3204023540019989 train acc 0.841343611805422\n","epoch 1 batch id 5401 loss 0.28710412979125977 train acc 0.8426622384743566\n","epoch 1 batch id 5601 loss 0.2656220495700836 train acc 0.8440205990001786\n","epoch 1 train acc 0.8451137443829935\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_368/1704445530.py:26: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c0927c341de34c80995d97f641d6edfd","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1935 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["epoch 1 test acc 0.8844159958258795\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8b09cbbed1b844708dbaf4b61e5a41d9","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5786 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["epoch 2 batch id 1 loss 0.3259432017803192 train acc 0.828125\n","epoch 2 batch id 201 loss 0.282256543636322 train acc 0.8755441542288557\n","epoch 2 batch id 401 loss 0.30092132091522217 train acc 0.8782730673316709\n","epoch 2 batch id 601 loss 0.33458057045936584 train acc 0.8786917637271214\n","epoch 2 batch id 801 loss 0.44746771454811096 train acc 0.8812031835205992\n","epoch 2 batch id 1001 loss 0.24357135593891144 train acc 0.8799793956043956\n","epoch 2 batch id 1201 loss 0.12974438071250916 train acc 0.8811667360532889\n","epoch 2 batch id 1401 loss 0.20059241354465485 train acc 0.8823162027123483\n","epoch 2 batch id 1601 loss 0.3207647502422333 train acc 0.8828759369144284\n","epoch 2 batch id 1801 loss 0.3664592504501343 train acc 0.8835629511382566\n","epoch 2 batch id 2001 loss 0.32676389813423157 train acc 0.8845264867566217\n","epoch 2 batch id 2201 loss 0.12741760909557343 train acc 0.8855420831440255\n","epoch 2 batch id 2401 loss 0.4138714075088501 train acc 0.8867008538109121\n","epoch 2 batch id 2601 loss 0.20727986097335815 train acc 0.8877354863514033\n","epoch 2 batch id 2801 loss 0.2694784998893738 train acc 0.888360183862906\n","epoch 2 batch id 3001 loss 0.2996666729450226 train acc 0.8892869043652116\n","epoch 2 batch id 3201 loss 0.2182425856590271 train acc 0.8899465010934083\n","epoch 2 batch id 3401 loss 0.3341333270072937 train acc 0.8906479711849457\n","epoch 2 batch id 3601 loss 0.1112295463681221 train acc 0.8915231880033324\n","epoch 2 batch id 3801 loss 0.3475979268550873 train acc 0.8925817219152854\n","epoch 2 batch id 4001 loss 0.18383322656154633 train acc 0.8933625968507873\n","epoch 2 batch id 4201 loss 0.2465578317642212 train acc 0.8939575398714592\n","epoch 2 batch id 4401 loss 0.28468412160873413 train acc 0.8945161610997501\n","epoch 2 batch id 4601 loss 0.33510783314704895 train acc 0.895029613127581\n","epoch 2 batch id 4801 loss 0.19439880549907684 train acc 0.8957931941262237\n","epoch 2 batch id 5001 loss 0.1914820671081543 train acc 0.8963832233553289\n","epoch 2 batch id 5201 loss 0.2503613233566284 train acc 0.8969489040569121\n","epoch 2 batch id 5401 loss 0.21053600311279297 train acc 0.897365649879652\n","epoch 2 batch id 5601 loss 0.20692099630832672 train acc 0.8979339626852347\n","epoch 2 train acc 0.8983780893536122\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"80e202c50b534af7baac9785d3fb055b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1935 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["epoch 2 test acc 0.889470905386603\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"217a062b729844808836afe00a7733ef","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5786 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["epoch 3 batch id 1 loss 0.26829850673675537 train acc 0.90625\n","epoch 3 batch id 201 loss 0.1758551150560379 train acc 0.9078824626865671\n","epoch 3 batch id 401 loss 0.3089134991168976 train acc 0.9102244389027432\n","epoch 3 batch id 601 loss 0.3068436086177826 train acc 0.9097337770382695\n","epoch 3 batch id 801 loss 0.36449918150901794 train acc 0.9124531835205992\n","epoch 3 batch id 1001 loss 0.28089720010757446 train acc 0.9121971778221778\n","epoch 3 batch id 1201 loss 0.0757289007306099 train acc 0.9137307452123231\n","epoch 3 batch id 1401 loss 0.0964483842253685 train acc 0.9146368665239115\n","epoch 3 batch id 1601 loss 0.1885727047920227 train acc 0.9150042941911305\n","epoch 3 batch id 1801 loss 0.26732781529426575 train acc 0.9160449056079956\n","epoch 3 batch id 2001 loss 0.2702658474445343 train acc 0.9169243503248375\n","epoch 3 batch id 2201 loss 0.05609556660056114 train acc 0.9178427419354839\n","epoch 3 batch id 2401 loss 0.2967988848686218 train acc 0.9189530403998334\n","epoch 3 batch id 2601 loss 0.1815025955438614 train acc 0.9197784505959247\n","epoch 3 batch id 2801 loss 0.18533103168010712 train acc 0.9204413602284899\n","epoch 3 batch id 3001 loss 0.20676599442958832 train acc 0.9213387204265245\n","epoch 3 batch id 3201 loss 0.17165742814540863 train acc 0.9220458450484224\n","epoch 3 batch id 3401 loss 0.23289959132671356 train acc 0.9228995148485739\n","epoch 3 batch id 3601 loss 0.0673016831278801 train acc 0.9234847958900305\n","epoch 3 batch id 3801 loss 0.23456619679927826 train acc 0.9243167916337806\n","epoch 3 batch id 4001 loss 0.1400732845067978 train acc 0.9248703449137715\n","epoch 3 batch id 4201 loss 0.2586972117424011 train acc 0.9252968043323019\n","epoch 3 batch id 4401 loss 0.1634068638086319 train acc 0.9257484094523972\n","epoch 3 batch id 4601 loss 0.23223479092121124 train acc 0.9262354651162791\n","epoch 3 batch id 4801 loss 0.1446191966533661 train acc 0.9268804676109144\n","epoch 3 batch id 5001 loss 0.14310242235660553 train acc 0.9274582583483303\n","epoch 3 batch id 5201 loss 0.11619069427251816 train acc 0.9279525572005384\n","epoch 3 batch id 5401 loss 0.1071530133485794 train acc 0.9283581744121459\n","epoch 3 batch id 5601 loss 0.13912777602672577 train acc 0.9287348241385467\n","epoch 3 train acc 0.9289691712754926\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"74b7d4b4643d43d8893a3b531589e366","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1935 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["epoch 3 test acc 0.8911343420791095\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7193267578c1462d9092636f34599ad8","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5786 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["epoch 4 batch id 1 loss 0.2051255851984024 train acc 0.921875\n","epoch 4 batch id 201 loss 0.15975570678710938 train acc 0.9402207711442786\n","epoch 4 batch id 401 loss 0.11895418912172318 train acc 0.9399937655860349\n","epoch 4 batch id 601 loss 0.17517535388469696 train acc 0.9394498752079867\n","epoch 4 batch id 801 loss 0.2579634487628937 train acc 0.9418695380774033\n","epoch 4 batch id 1001 loss 0.1779208481311798 train acc 0.9418706293706294\n","epoch 4 batch id 1201 loss 0.04537121206521988 train acc 0.9429641965029142\n","epoch 4 batch id 1401 loss 0.08620116859674454 train acc 0.9433774982155603\n","epoch 4 batch id 1601 loss 0.2172497808933258 train acc 0.9439998438475953\n","epoch 4 batch id 1801 loss 0.2010403424501419 train acc 0.9445360216546363\n","epoch 4 batch id 2001 loss 0.1623809039592743 train acc 0.9452617441279361\n","epoch 4 batch id 2201 loss 0.021287502720952034 train acc 0.9461253407542026\n","epoch 4 batch id 2401 loss 0.23723889887332916 train acc 0.9469426801332778\n","epoch 4 batch id 2601 loss 0.1436348408460617 train acc 0.9475141772395232\n","epoch 4 batch id 2801 loss 0.11421384662389755 train acc 0.9480654230631917\n","epoch 4 batch id 3001 loss 0.19795362651348114 train acc 0.9485848467177608\n","epoch 4 batch id 3201 loss 0.09517907351255417 train acc 0.9491321071540144\n","epoch 4 batch id 3401 loss 0.1293119192123413 train acc 0.949706887680094\n","epoch 4 batch id 3601 loss 0.02146938070654869 train acc 0.9501353790613718\n","epoch 4 batch id 3801 loss 0.13966448605060577 train acc 0.9507448697711128\n","epoch 4 batch id 4001 loss 0.06761052459478378 train acc 0.9511762684328918\n","epoch 4 batch id 4201 loss 0.17697514593601227 train acc 0.9514364139490598\n","epoch 4 batch id 4401 loss 0.13167434930801392 train acc 0.9517616734832992\n","epoch 4 batch id 4601 loss 0.16493572294712067 train acc 0.9520722397304934\n","epoch 4 batch id 4801 loss 0.0728657990694046 train acc 0.952461075817538\n","epoch 4 batch id 5001 loss 0.09913024306297302 train acc 0.9528375574885023\n","epoch 4 batch id 5201 loss 0.12032729387283325 train acc 0.9531009661603538\n","epoch 4 batch id 5401 loss 0.052921541035175323 train acc 0.9532841140529531\n","epoch 4 batch id 5601 loss 0.10012204945087433 train acc 0.9536355115158007\n","epoch 4 train acc 0.9537650146906326\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b9ce39c5571743149345513ac3200f3c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1935 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["epoch 4 test acc 0.8903305754323196\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2ea275c1b82945fc85508f93188506b4","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5786 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["epoch 5 batch id 1 loss 0.1908959597349167 train acc 0.9375\n","epoch 5 batch id 201 loss 0.1060066819190979 train acc 0.9614427860696517\n","epoch 5 batch id 401 loss 0.0872025117278099 train acc 0.9601387157107232\n","epoch 5 batch id 601 loss 0.13215786218643188 train acc 0.9594165973377704\n","epoch 5 batch id 801 loss 0.20559781789779663 train acc 0.9611423220973783\n","epoch 5 batch id 1001 loss 0.1769813597202301 train acc 0.9608204295704296\n","epoch 5 batch id 1201 loss 0.010505618527531624 train acc 0.96189373438801\n","epoch 5 batch id 1401 loss 0.0377800352871418 train acc 0.9625156138472519\n","epoch 5 batch id 1601 loss 0.05036428943276405 train acc 0.9629918800749532\n","epoch 5 batch id 1801 loss 0.057756535708904266 train acc 0.9635705857856747\n","epoch 5 batch id 2001 loss 0.09551557153463364 train acc 0.9636509870064968\n","epoch 5 batch id 2201 loss 0.008611119352281094 train acc 0.9640291344843253\n","epoch 5 batch id 2401 loss 0.30358678102493286 train acc 0.9644614223240316\n","epoch 5 batch id 2601 loss 0.09991064667701721 train acc 0.9646650326797386\n","epoch 5 batch id 2801 loss 0.08847229182720184 train acc 0.9647726258479115\n","epoch 5 batch id 3001 loss 0.1050758957862854 train acc 0.9650376957680773\n","epoch 5 batch id 3201 loss 0.12658849358558655 train acc 0.9652989300218682\n","epoch 5 batch id 3401 loss 0.13714252412319183 train acc 0.9654880917377242\n","epoch 5 batch id 3601 loss 0.007471076678484678 train acc 0.9655260691474591\n","epoch 5 batch id 3801 loss 0.06115621700882912 train acc 0.9657573664825047\n","epoch 5 batch id 4001 loss 0.0407235324382782 train acc 0.9660006873281679\n","epoch 5 batch id 4201 loss 0.20962278544902802 train acc 0.9660832242323256\n","epoch 5 batch id 4401 loss 0.07488753646612167 train acc 0.9661724608043627\n","epoch 5 batch id 4601 loss 0.09983696043491364 train acc 0.9662573353618779\n","epoch 5 batch id 4801 loss 0.05088644474744797 train acc 0.9663774474067902\n","epoch 5 batch id 5001 loss 0.05399800091981888 train acc 0.9665129474105179\n","epoch 5 batch id 5201 loss 0.10459422320127487 train acc 0.9665809459719285\n","epoch 5 batch id 5401 loss 0.13012559711933136 train acc 0.9666323366043326\n","epoch 5 batch id 5601 loss 0.07735887914896011 train acc 0.966760957864667\n","epoch 5 train acc 0.9667786467334947\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"09fe54ac1c744389803369807a8aa7ed","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1935 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["epoch 5 test acc 0.8918163635460148\n"]}],"source":["# 모델 학습 시작\n","for e in range(num_epochs):\n","    train_acc = 0.0\n","    test_acc = 0.0\n","    \n","    model.train()\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n","        optimizer.zero_grad()\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length= valid_length\n","        label = label.long().to(device)\n","        out = model(token_ids, valid_length, segment_ids)\n","        loss = loss_fn(out, label)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm) # gradient clipping\n","        optimizer.step()\n","        scheduler.step()  # Update learning rate schedule\n","        train_acc += calc_accuracy(out, label)\n","        if batch_id % log_interval == 0:\n","            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n","    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n","    \n","    model.eval() # 평가 모드로 변경\n","    \n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length= valid_length\n","        label = label.long().to(device)\n","        out = model(token_ids, valid_length, segment_ids)\n","        test_acc += calc_accuracy(out, label)\n","    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"]},{"cell_type":"markdown","metadata":{"id":"TRIwc4JKfkIq"},"source":["## 9. 테스트"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sl_VEH3Ufp2d"},"outputs":[],"source":["# 테스트 문장 예측\n","test_sentence = '무난하네요.'\n","test_label = 1 # 실제 정답"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HGzKcMLk5Vwk"},"outputs":[],"source":["unseen_test = pd.DataFrame([[test_sentence, test_label]], columns = [['document', 'label']])\n","# unseen_values = unseen_test.values\n","test_set = BERTDataset(unseen_test, 1, 2, tok, max_len, True, False)\n","test_input = torch.utils.data.DataLoader(test_set, batch_size=1, num_workers=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["669acb25d23545d597d6c64b0b5fe9f7"]},"id":"_yDWhv1j5W-m","outputId":"e9898cfc-10a7-4f0b-994a-6f0405ca4fdf"},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_368/264004550.py:1: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_input)):\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"669acb25d23545d597d6c64b0b5fe9f7","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["tensor([[-0.1702,  0.1392]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"]}],"source":["for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_input)):\n","    token_ids = token_ids.long().to(device)\n","    segment_ids = segment_ids.long().to(device)\n","    valid_length= valid_length\n","    out = model(token_ids, valid_length, segment_ids)\n","    print(out)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ERrm-OcppQCi","outputId":"497b3f6e-8266-4327-c39b-262dcc5d1c73"},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_368/951339506.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  result = torch.nn.functional.softmax(out)\n"]},{"data":{"text/plain":["tensor([[0.4233, 0.5767]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["result = torch.nn.functional.softmax(out)\n","result"]},{"cell_type":"markdown","metadata":{"id":"eoZXpyZMjFXH"},"source":["## 10. 모델 저장 및 불러오기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NM--jVKv5dI9"},"outputs":[],"source":["torch.save(model.state_dict(), \"./best_weights_v2.pt\")\n","torch.save(model, \"./best_v2.pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"acPJGKr-S0FV"},"outputs":[],"source":["loaded_model = torch.load('best_v2.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fIP2lZKvpQCi"},"outputs":[],"source":["# 테스트 문장 예측\n","test_sentence = '너무 별로에요, 사지마세요. 돈낭비입니다.'\n","test_label = 0 # 실제 정답"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LxXlokbJpQCi"},"outputs":[],"source":["unseen_test = pd.DataFrame([[test_sentence, test_label]], columns = [['document', 'label']])\n","# unseen_values = unseen_test.values\n","test_set = BERTDataset(unseen_test, 1, 2, tok, max_len, True, False)\n","test_input = torch.utils.data.DataLoader(test_set, batch_size=1, num_workers=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["da1e214ae79645c494b89d5a05ea3258"]},"id":"5cyUFFGEpQCj","outputId":"494ab64c-9b00-4955-c75e-821ace6dc379"},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_368/809299586.py:1: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_input)):\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"da1e214ae79645c494b89d5a05ea3258","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["tensor([[ 3.7407, -3.1388]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"]}],"source":["for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_input)):\n","    token_ids = token_ids.long().to(device)\n","    segment_ids = segment_ids.long().to(device)\n","    valid_length= valid_length\n","    out = loaded_model(token_ids, valid_length, segment_ids)\n","    print(out)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MpBH8Pq-pQCj","outputId":"b9be5a42-3c4d-4a0d-ee98-cebbb42faebd"},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_368/951339506.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  result = torch.nn.functional.softmax(out)\n"]},{"data":{"text/plain":["tensor([[0.9990, 0.0010]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["result = torch.nn.functional.softmax(out)\n","result"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}
